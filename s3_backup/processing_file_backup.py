"""
S3 Backup Manager for Processing Session Files (Individual File Backup)
Backs up intermediate and final files individually rather than as tarballs.
"""

import hashlib
import logging
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime

import boto3
from botocore.exceptions import ClientError
from tqdm import tqdm

from models import ProcessingSession
from processed_catalog.models import ProcessedFile
from s3_backup.models import S3BackupProcessedFileRecord
from models import DatabaseManager

logger = logging.getLogger(__name__)


@dataclass
class FileBackupResult:
    """Result of backing up a single file."""
    success: bool
    file_path: str
    s3_key: Optional[str] = None
    s3_etag: Optional[str] = None
    file_size: int = 0
    md5sum: Optional[str] = None
    needs_backup: bool = True
    reason: str = ""
    error: Optional[str] = None


class UploadProgressCallback:
    """Callback for S3 upload progress."""
    def __init__(self, file_num: int, total_files: int, file_size: int):
        self.pbar = tqdm(
            total=file_size,
            unit='B',
            unit_scale=True,
            desc=f"File {file_num}/{total_files}"
        )
    
    def __call__(self, bytes_transferred):
        self.pbar.update(bytes_transferred)
    
    def close(self):
        self.pbar.close()


class ProcessingSessionFileBackup:
    """Manages individual file backups for processing sessions to S3."""
    
    def __init__(self, config, s3_config, db_service):
        """
        Initialize backup manager.
        
        Args:
            config: Application configuration
            s3_config: S3BackupConfig object (from s3_backup.manager)
            db_service: Database service
        """
        self.config = config
        self.s3_config = s3_config
        self.db_service = db_service
        
        # Initialize S3 client
        self.s3_client = boto3.client(
            's3',
            region_name=s3_config.region
        )
        self.bucket = s3_config.bucket
        
    def calculate_md5(self, filepath: Path) -> str:
        """Calculate MD5 hash of a file."""
        hash_md5 = hashlib.md5()
        try:
            with open(filepath, "rb") as f:
                for chunk in iter(lambda: f.read(8192), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            logger.error(f"Error calculating MD5 for {filepath}: {e}")
            return ""
    
    def _get_file_s3_key(self, session_id: str, subfolder: str, filename: str) -> str:
        """
        Generate S3 key for a processing session file.
        
        Structure: backups/processed/YEAR/SESSION_ID/subfolder/filename
        Example: backups/processed/2025/20250115_ABC123/final/M31_final.xisf
        """
        try:
            year = int(session_id[:4])
        except (ValueError, IndexError):
            year = datetime.now().year
        
        return f"backups/processed/{year}/{session_id}/{subfolder}/{filename}"
    
    def check_file_needs_backup(self, file_record: ProcessedFile) -> Tuple[bool, str]:
        """
        Check if a file needs to be backed up.
        
        Returns:
            (needs_backup, reason)
        """
        session = self.db_service.db_manager.get_session()
        
        try:
            backup_record = session.query(S3BackupProcessedFileRecord).filter(
                S3BackupProcessedFileRecord.processed_file_id == file_record.id
            ).first()
            
            if not backup_record:
                return True, "No backup record exists"
            
            file_path = Path(file_record.file_path)
            if not file_path.exists():
                return False, "Source file not found locally"
            
            current_md5 = file_record.md5sum
            if not current_md5:
                current_md5 = self.calculate_md5(file_path)
                file_record.md5sum = current_md5
                session.commit()
            
            if backup_record.md5sum != current_md5:
                return True, f"File modified (old: {backup_record.md5sum[:8]}..., new: {current_md5[:8]}...)"
            
            current_size = file_path.stat().st_size
            if backup_record.file_size != current_size:
                return True, f"File size changed ({backup_record.file_size} -> {current_size})"
            
            return False, "File already backed up and unchanged"
            
        finally:
            session.close()
    
    def backup_file(
        self,
        file_record: ProcessedFile,
        force: bool = False,
        file_num: int = 1,
        total_files: int = 1
    ) -> FileBackupResult:
        """Backup a single processed file to S3."""
        file_path = Path(file_record.file_path)
        
        if not file_path.exists():
            return FileBackupResult(
                success=False,
                file_path=str(file_path),
                error=f"File not found: {file_path}"
            )
        
        if not force:
            needs_backup, reason = self.check_file_needs_backup(file_record)
            if not needs_backup:
                return FileBackupResult(
                    success=True,
                    file_path=str(file_path),
                    needs_backup=False,
                    reason=reason
                )
        
        if not file_record.md5sum:
            file_record.md5sum = self.calculate_md5(file_path)
            session = self.db_service.db_manager.get_session()
            session.merge(file_record)
            session.commit()
            session.close()
        
        s3_key = self._get_file_s3_key(
            file_record.processing_session_id,
            file_record.subfolder,
            file_record.filename
        )
        
        file_size = file_path.stat().st_size
        
        try:
            logger.info(f"Uploading {file_record.filename} to S3...")
            
            # Load backup rules from config
            rules = self.s3_config.config.get('backup_rules', {}).get('final_outputs', {})
            archive_policy = rules.get('archive_policy', 'standard')
            backup_policy = rules.get('backup_policy', 'flexible')
            tags = f"archive_policy={archive_policy}&backup_policy={backup_policy}"
            
            # Determine content type
            content_type_map = {
                'xisf': 'application/octet-stream',
                'jpg': 'image/jpeg',
                'jpeg': 'image/jpeg',
                'xosm': 'application/octet-stream',
                'pxiproject': 'application/octet-stream'
            }
            content_type = content_type_map.get(file_record.file_type, 'application/octet-stream')
            file_size = file_path.stat().st_size
            progress_callback = UploadProgressCallback(file_num, total_files, file_size)
            
            # Upload with tags
            with open(file_path, 'rb') as f:
                upload_kwargs = {
                    'ExtraArgs': {
                        'ContentType': content_type,
                        'Tagging': tags
                    }
                }
                
                if progress_callback:
                    upload_kwargs['Callback'] = progress_callback
                
                self.s3_client.upload_fileobj(
                    f,
                    self.bucket,
                    s3_key,
                    **upload_kwargs
                )
                
            response = self.s3_client.head_object(
                Bucket=self.bucket,
                Key=s3_key
            )
            etag = response['ETag'].strip('"')
            
            self._update_backup_record(
                file_record,
                s3_key,
                etag,
                file_size
            )
            
            logger.info(f"âœ“ Uploaded: {file_record.filename}")
            
            return FileBackupResult(
                success=True,
                file_path=str(file_path),
                s3_key=s3_key,
                s3_etag=etag,
                file_size=file_size,
                md5sum=file_record.md5sum,
                needs_backup=True,
                reason="Uploaded successfully"
            )
            
        except ClientError as e:
            logger.error(f"Failed to upload {file_record.filename}: {e}")
            return FileBackupResult(
                success=False,
                file_path=str(file_path),
                error=f"S3 upload failed: {e}"
            )
        except Exception as e:
            logger.error(f"Unexpected error uploading {file_record.filename}: {e}")
            return FileBackupResult(
                success=False,
                file_path=str(file_path),
                error=str(e)
            )
    
    def _update_backup_record(
        self,
        file_record: ProcessedFile,
        s3_key: str,
        etag: str,
        file_size: int
    ):
        """Update or create backup record in database."""
        session = self.db_service.db_manager.get_session()
        
        try:
            backup_record = session.query(S3BackupProcessedFileRecord).filter(
                S3BackupProcessedFileRecord.processed_file_id == file_record.id
            ).first()
            
            if backup_record:
                backup_record.s3_key = s3_key
                backup_record.s3_etag = etag
                backup_record.s3_bucket = self.bucket
                backup_record.s3_region = self.s3_config.region
                backup_record.file_size = file_size
                backup_record.md5sum = file_record.md5sum
                backup_record.uploaded_at = datetime.utcnow()
            else:
                backup_record = S3BackupProcessedFileRecord(
                    processed_file_id=file_record.id,
                    processing_session_id=file_record.processing_session_id,
                    s3_bucket=self.bucket,
                    s3_key=s3_key,
                    s3_region=self.s3_config.region,
                    s3_etag=etag,
                    file_size=file_size,
                    md5sum=file_record.md5sum,
                    uploaded_at=datetime.utcnow()
                )
                session.add(backup_record)
            
            session.commit()
            
        finally:
            session.close()
    
    def backup_session_files(
        self,
        session_id: str,
        subfolders: Optional[List[str]] = None,
        file_types: Optional[List[str]] = None,
        force: bool = False
    ) -> Dict:
        """Backup all files from a processing session."""
        session = self.db_service.db_manager.get_session()
        
        try:
            query = session.query(ProcessedFile).filter(
                ProcessedFile.processing_session_id == session_id
            )
            
            if subfolders:
                query = query.filter(ProcessedFile.subfolder.in_(subfolders))
            
            if file_types:
                query = query.filter(ProcessedFile.file_type.in_(file_types))
            
            files = query.all()
            
            if not files:
                logger.warning(f"No files found for session {session_id}")
                return {
                    'success': True,
                    'total_files': 0,
                    'uploaded': 0,
                    'skipped': 0,
                    'failed': 0,
                    'total_size': 0
                }
            
            stats = {
                'total_files': len(files),
                'uploaded': 0,
                'skipped': 0,
                'failed': 0,
                'total_size': 0,
                'errors': []
            }
            
            logger.info(f"Backing up {len(files)} files from session {session_id}...")
            
            with tqdm(total=len(files), desc="Backing up files", unit="files") as pbar:
                for idx, file_record in enumerate(files, 1):
                    result = self.backup_file(file_record, force=force, file_num=idx, total_files=len(files))
                    
                    if result.success:
                        if result.needs_backup:
                            stats['uploaded'] += 1
                            stats['total_size'] += result.file_size
                        else:
                            stats['skipped'] += 1
                    else:
                        stats['failed'] += 1
                        stats['errors'].append({
                            'file': file_record.filename,
                            'error': result.error
                        })
                    
                    pbar.update(1)
                    pbar.set_postfix({
                        'uploaded': stats['uploaded'],
                        'skipped': stats['skipped'],
                        'failed': stats['failed']
                    })
            
            stats['success'] = stats['failed'] == 0
            
            logger.info(f"Backup complete for session {session_id}:")
            logger.info(f"  Uploaded: {stats['uploaded']}")
            logger.info(f"  Skipped: {stats['skipped']}")
            logger.info(f"  Failed: {stats['failed']}")
            logger.info(f"  Total size: {self._format_bytes(stats['total_size'])}")
            
            return stats
            
        finally:
            session.close()
    
    def verify_backup(self, file_record: ProcessedFile) -> bool:
        """Verify that a file's backup matches the local file."""
        session = self.db_service.db_manager.get_session()
        
        try:
            backup_record = session.query(S3BackupProcessedFileRecord).filter(
                S3BackupProcessedFileRecord.processed_file_id == file_record.id
            ).first()
            
            if not backup_record:
                logger.warning(f"No backup record for {file_record.filename}")
                return False
            
            try:
                response = self.s3_client.head_object(
                    Bucket=backup_record.s3_bucket,
                    Key=backup_record.s3_key
                )
                
                s3_etag = response['ETag'].strip('"')
                
                if s3_etag != backup_record.s3_etag:
                    logger.error(f"ETag mismatch for {file_record.filename}")
                    return False
                
                s3_size = response['ContentLength']
                if s3_size != backup_record.file_size:
                    logger.error(f"Size mismatch for {file_record.filename}")
                    return False
                
                return True
                
            except ClientError as e:
                if e.response['Error']['Code'] == '404':
                    logger.error(f"File not found in S3: {backup_record.s3_key}")
                else:
                    logger.error(f"Error checking S3: {e}")
                return False
                
        finally:
            session.close()
    
    def list_session_backups(self, session_id: str) -> Dict:
        """List all backed up files for a processing session."""
        session = self.db_service.db_manager.get_session()
        
        try:
            backup_records = session.query(S3BackupProcessedFileRecord).filter(
                S3BackupProcessedFileRecord.processing_session_id == session_id
            ).all()
            
            if not backup_records:
                return {
                    'session_id': session_id,
                    'total_files': 0,
                    'total_size': 0,
                    'files': []
                }
            
            files = []
            total_size = 0
            
            for record in backup_records:
                file_info = session.query(ProcessedFile).filter(
                    ProcessedFile.id == record.processed_file_id
                ).first()
                
                if file_info:
                    files.append({
                        'filename': file_info.filename,
                        'subfolder': file_info.subfolder,
                        'file_type': file_info.file_type,
                        's3_key': record.s3_key,
                        'file_size': record.file_size,
                        'uploaded_at': record.uploaded_at,
                        'md5sum': record.md5sum
                    })
                    total_size += record.file_size or 0
            
            return {
                'session_id': session_id,
                'total_files': len(files),
                'total_size': total_size,
                'total_size_mb': round(total_size / 1024 / 1024, 2),
                'files': sorted(files, key=lambda x: x['subfolder'] + x['filename'])
            }
            
        finally:
            session.close()
    
    def _format_bytes(self, size: int) -> str:
        """Format bytes to human readable format."""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size < 1024.0:
                return f"{size:.2f} {unit}"
            size /= 1024.0
        return f"{size:.2f} PB"